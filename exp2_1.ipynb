{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0229d18",
   "metadata": {},
   "source": [
    "(1) 데이터 가져오기\n",
    "\n",
    " sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.\n",
    " \n",
    " diabetes의 data를 df_X에, target을 df_y에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8588a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990842, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06832974, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286377, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04687948,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452837, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00421986,  0.00306441]]), 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.]), 'frame': None, 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, total serum cholesterol\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, total cholesterol / HDL\\n      - s5      ltg, possibly log of serum triglycerides level\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)', 'feature_names': ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], 'data_filename': 'diabetes_data.csv.gz', 'target_filename': 'diabetes_target.csv.gz', 'data_module': 'sklearn.datasets.data'}\n",
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019908 -0.017646   151.0  \n",
      "1 -0.039493 -0.068330 -0.092204    75.0  \n",
      "2 -0.002592  0.002864 -0.025930   141.0  \n",
      "3  0.034309  0.022692 -0.009362   206.0  \n",
      "4 -0.002592 -0.031991 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "print(diabetes)\n",
    "df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "print(df.head())\n",
    "# df_X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "# df_y = pd.DataFrame(diabetes.target, columns=['target'])\n",
    "\n",
    "# df_X = pd.DataFrame(diabetes.data)\n",
    "# df_y = pd.DataFrame(diabetes.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf537b",
   "metadata": {},
   "source": [
    "(2) 모델에 입력할 데이터 X 준비하기\n",
    "\n",
    "df_X에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a68e2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = diabetes.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2129565",
   "metadata": {},
   "source": [
    "(3) 모델에 예측할 데이터 y 준비하기\n",
    "\n",
    "df_y에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceb51143",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32934239",
   "metadata": {},
   "source": [
    "(4) train 데이터와 test 데이터로 분리하기\n",
    "\n",
    "X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "031a92a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4a1ad",
   "metadata": {},
   "source": [
    "(5) 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14a4180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2e6ca",
   "metadata": {},
   "source": [
    "(6) 손실함수 loss 정의하기\n",
    "\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a2e58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "#     rmse = mse ** 0.5        # MSE의 제곱근\n",
    "#     return rmse\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7c8d4",
   "metadata": {},
   "source": [
    "(7) 기울기를 구하는 gradient 함수 구현하기\n",
    "\n",
    "기울기를 계산하는 gradient 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09f64ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [-1.36869319 -0.31097871 -4.28800997 -3.22498304 -1.5440604  -1.26655487\n",
      "  2.88771775 -3.1445484  -4.13647513 -2.79215225]\n",
      "db: -303.89890037277763\n"
     ]
    }
   ],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "   \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "\n",
    "dW, db = gradient(X, W, b, y)\n",
    "print(\"dW:\", dW)\n",
    "print(\"db:\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93b61c",
   "metadata": {},
   "source": [
    "(8) 하이퍼 파라미터인 학습률 설정하기\n",
    "\n",
    "학습률, learning rate 를 설정해주세요\n",
    "\n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "746fb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08bb88",
   "metadata": {},
   "source": [
    "(9) 모델 학습하기\n",
    "\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "\n",
    "입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fecb8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 5705.6149\n",
      "Iteration 20 : Loss 4962.6894\n",
      "Iteration 30 : Loss 4597.9875\n",
      "Iteration 40 : Loss 4319.1782\n",
      "Iteration 50 : Loss 4101.7260\n",
      "Iteration 60 : Loss 3929.9426\n",
      "Iteration 70 : Loss 3792.4076\n",
      "Iteration 80 : Loss 3680.7656\n",
      "Iteration 90 : Loss 3588.8829\n",
      "Iteration 100 : Loss 3512.2376\n",
      "Iteration 110 : Loss 3447.4798\n",
      "Iteration 120 : Loss 3392.1127\n",
      "Iteration 130 : Loss 3344.2622\n",
      "Iteration 140 : Loss 3302.5103\n",
      "Iteration 150 : Loss 3265.7739\n",
      "Iteration 160 : Loss 3233.2169\n",
      "Iteration 170 : Loss 3204.1867\n",
      "Iteration 180 : Loss 3178.1672\n",
      "Iteration 190 : Loss 3154.7451\n",
      "Iteration 200 : Loss 3133.5850\n",
      "Iteration 210 : Loss 3114.4109\n",
      "Iteration 220 : Loss 3096.9930\n",
      "Iteration 230 : Loss 3081.1374\n",
      "Iteration 240 : Loss 3066.6785\n",
      "Iteration 250 : Loss 3053.4739\n",
      "Iteration 260 : Loss 3041.3995\n",
      "Iteration 270 : Loss 3030.3465\n",
      "Iteration 280 : Loss 3020.2188\n",
      "Iteration 290 : Loss 3010.9313\n",
      "Iteration 300 : Loss 3002.4077\n",
      "Iteration 310 : Loss 2994.5798\n",
      "Iteration 320 : Loss 2987.3863\n",
      "Iteration 330 : Loss 2980.7719\n",
      "Iteration 340 : Loss 2974.6865\n",
      "Iteration 350 : Loss 2969.0847\n",
      "Iteration 360 : Loss 2963.9255\n",
      "Iteration 370 : Loss 2959.1715\n",
      "Iteration 380 : Loss 2954.7887\n",
      "Iteration 390 : Loss 2950.7460\n",
      "Iteration 400 : Loss 2947.0154\n",
      "Iteration 410 : Loss 2943.5710\n",
      "Iteration 420 : Loss 2940.3894\n",
      "Iteration 430 : Loss 2937.4491\n",
      "Iteration 440 : Loss 2934.7304\n",
      "Iteration 450 : Loss 2932.2155\n",
      "Iteration 460 : Loss 2929.8879\n",
      "Iteration 470 : Loss 2927.7326\n",
      "Iteration 480 : Loss 2925.7359\n",
      "Iteration 490 : Loss 2923.8852\n",
      "Iteration 500 : Loss 2922.1689\n",
      "Iteration 510 : Loss 2920.5765\n",
      "Iteration 520 : Loss 2919.0983\n",
      "Iteration 530 : Loss 2917.7254\n",
      "Iteration 540 : Loss 2916.4497\n",
      "Iteration 550 : Loss 2915.2635\n",
      "Iteration 560 : Loss 2914.1601\n",
      "Iteration 570 : Loss 2913.1332\n",
      "Iteration 580 : Loss 2912.1768\n",
      "Iteration 590 : Loss 2911.2858\n",
      "Iteration 600 : Loss 2910.4551\n",
      "Iteration 610 : Loss 2909.6802\n",
      "Iteration 620 : Loss 2908.9570\n",
      "Iteration 630 : Loss 2908.2817\n",
      "Iteration 640 : Loss 2907.6508\n",
      "Iteration 650 : Loss 2907.0609\n",
      "Iteration 660 : Loss 2906.5091\n",
      "Iteration 670 : Loss 2905.9926\n",
      "Iteration 680 : Loss 2905.5088\n",
      "Iteration 690 : Loss 2905.0555\n",
      "Iteration 700 : Loss 2904.6305\n",
      "Iteration 710 : Loss 2904.2316\n",
      "Iteration 720 : Loss 2903.8572\n",
      "Iteration 730 : Loss 2903.5055\n",
      "Iteration 740 : Loss 2903.1748\n",
      "Iteration 750 : Loss 2902.8638\n",
      "Iteration 760 : Loss 2902.5710\n",
      "Iteration 770 : Loss 2902.2953\n",
      "Iteration 780 : Loss 2902.0354\n",
      "Iteration 790 : Loss 2901.7903\n",
      "Iteration 800 : Loss 2901.5590\n",
      "Iteration 810 : Loss 2901.3405\n",
      "Iteration 820 : Loss 2901.1340\n",
      "Iteration 830 : Loss 2900.9388\n",
      "Iteration 840 : Loss 2900.7540\n",
      "Iteration 850 : Loss 2900.5790\n",
      "Iteration 860 : Loss 2900.4131\n",
      "Iteration 870 : Loss 2900.2558\n",
      "Iteration 880 : Loss 2900.1065\n",
      "Iteration 890 : Loss 2899.9647\n",
      "Iteration 900 : Loss 2899.8299\n",
      "Iteration 910 : Loss 2899.7016\n",
      "Iteration 920 : Loss 2899.5795\n",
      "Iteration 930 : Loss 2899.4632\n",
      "Iteration 940 : Loss 2899.3523\n",
      "Iteration 950 : Loss 2899.2465\n",
      "Iteration 960 : Loss 2899.1454\n",
      "Iteration 970 : Loss 2899.0488\n",
      "Iteration 980 : Loss 2898.9564\n",
      "Iteration 990 : Loss 2898.8679\n",
      "Iteration 1000 : Loss 2898.7832\n",
      "Iteration 1010 : Loss 2898.7019\n",
      "Iteration 1020 : Loss 2898.6239\n",
      "Iteration 1030 : Loss 2898.5490\n",
      "Iteration 1040 : Loss 2898.4771\n",
      "Iteration 1050 : Loss 2898.4078\n",
      "Iteration 1060 : Loss 2898.3411\n",
      "Iteration 1070 : Loss 2898.2769\n",
      "Iteration 1080 : Loss 2898.2149\n",
      "Iteration 1090 : Loss 2898.1551\n",
      "Iteration 1100 : Loss 2898.0974\n",
      "Iteration 1110 : Loss 2898.0415\n",
      "Iteration 1120 : Loss 2897.9875\n",
      "Iteration 1130 : Loss 2897.9351\n",
      "Iteration 1140 : Loss 2897.8844\n",
      "Iteration 1150 : Loss 2897.8353\n",
      "Iteration 1160 : Loss 2897.7875\n",
      "Iteration 1170 : Loss 2897.7411\n",
      "Iteration 1180 : Loss 2897.6961\n",
      "Iteration 1190 : Loss 2897.6522\n",
      "Iteration 1200 : Loss 2897.6095\n",
      "Iteration 1210 : Loss 2897.5678\n",
      "Iteration 1220 : Loss 2897.5272\n",
      "Iteration 1230 : Loss 2897.4876\n",
      "Iteration 1240 : Loss 2897.4489\n",
      "Iteration 1250 : Loss 2897.4111\n",
      "Iteration 1260 : Loss 2897.3742\n",
      "Iteration 1270 : Loss 2897.3380\n",
      "Iteration 1280 : Loss 2897.3025\n",
      "Iteration 1290 : Loss 2897.2678\n",
      "Iteration 1300 : Loss 2897.2337\n",
      "Iteration 1310 : Loss 2897.2003\n",
      "Iteration 1320 : Loss 2897.1675\n",
      "Iteration 1330 : Loss 2897.1353\n",
      "Iteration 1340 : Loss 2897.1036\n",
      "Iteration 1350 : Loss 2897.0724\n",
      "Iteration 1360 : Loss 2897.0417\n",
      "Iteration 1370 : Loss 2897.0115\n",
      "Iteration 1380 : Loss 2896.9817\n",
      "Iteration 1390 : Loss 2896.9524\n",
      "Iteration 1400 : Loss 2896.9234\n",
      "Iteration 1410 : Loss 2896.8949\n",
      "Iteration 1420 : Loss 2896.8667\n",
      "Iteration 1430 : Loss 2896.8388\n",
      "Iteration 1440 : Loss 2896.8113\n",
      "Iteration 1450 : Loss 2896.7841\n",
      "Iteration 1460 : Loss 2896.7572\n",
      "Iteration 1470 : Loss 2896.7306\n",
      "Iteration 1480 : Loss 2896.7043\n",
      "Iteration 1490 : Loss 2896.6782\n",
      "Iteration 1500 : Loss 2896.6523\n",
      "Iteration 1510 : Loss 2896.6267\n",
      "Iteration 1520 : Loss 2896.6014\n",
      "Iteration 1530 : Loss 2896.5762\n",
      "Iteration 1540 : Loss 2896.5513\n",
      "Iteration 1550 : Loss 2896.5265\n",
      "Iteration 1560 : Loss 2896.5019\n",
      "Iteration 1570 : Loss 2896.4775\n",
      "Iteration 1580 : Loss 2896.4533\n",
      "Iteration 1590 : Loss 2896.4293\n",
      "Iteration 1600 : Loss 2896.4054\n",
      "Iteration 1610 : Loss 2896.3816\n",
      "Iteration 1620 : Loss 2896.3580\n",
      "Iteration 1630 : Loss 2896.3345\n",
      "Iteration 1640 : Loss 2896.3112\n",
      "Iteration 1650 : Loss 2896.2880\n",
      "Iteration 1660 : Loss 2896.2649\n",
      "Iteration 1670 : Loss 2896.2419\n",
      "Iteration 1680 : Loss 2896.2191\n",
      "Iteration 1690 : Loss 2896.1963\n",
      "Iteration 1700 : Loss 2896.1736\n",
      "Iteration 1710 : Loss 2896.1511\n",
      "Iteration 1720 : Loss 2896.1286\n",
      "Iteration 1730 : Loss 2896.1062\n",
      "Iteration 1740 : Loss 2896.0839\n",
      "Iteration 1750 : Loss 2896.0617\n",
      "Iteration 1760 : Loss 2896.0396\n",
      "Iteration 1770 : Loss 2896.0176\n",
      "Iteration 1780 : Loss 2895.9956\n",
      "Iteration 1790 : Loss 2895.9737\n",
      "Iteration 1800 : Loss 2895.9519\n",
      "Iteration 1810 : Loss 2895.9301\n",
      "Iteration 1820 : Loss 2895.9084\n",
      "Iteration 1830 : Loss 2895.8867\n",
      "Iteration 1840 : Loss 2895.8652\n",
      "Iteration 1850 : Loss 2895.8436\n",
      "Iteration 1860 : Loss 2895.8222\n",
      "Iteration 1870 : Loss 2895.8008\n",
      "Iteration 1880 : Loss 2895.7794\n",
      "Iteration 1890 : Loss 2895.7581\n",
      "Iteration 1900 : Loss 2895.7368\n",
      "Iteration 1910 : Loss 2895.7156\n",
      "Iteration 1920 : Loss 2895.6944\n",
      "Iteration 1930 : Loss 2895.6733\n",
      "Iteration 1940 : Loss 2895.6522\n",
      "Iteration 1950 : Loss 2895.6312\n",
      "Iteration 1960 : Loss 2895.6102\n",
      "Iteration 1970 : Loss 2895.5892\n",
      "Iteration 1980 : Loss 2895.5683\n",
      "Iteration 1990 : Loss 2895.5474\n",
      "Iteration 2000 : Loss 2895.5266\n",
      "Iteration 2010 : Loss 2895.5058\n",
      "Iteration 2020 : Loss 2895.4850\n",
      "Iteration 2030 : Loss 2895.4642\n",
      "Iteration 2040 : Loss 2895.4435\n",
      "Iteration 2050 : Loss 2895.4228\n",
      "Iteration 2060 : Loss 2895.4022\n",
      "Iteration 2070 : Loss 2895.3816\n",
      "Iteration 2080 : Loss 2895.3610\n",
      "Iteration 2090 : Loss 2895.3404\n",
      "Iteration 2100 : Loss 2895.3199\n",
      "Iteration 2110 : Loss 2895.2994\n",
      "Iteration 2120 : Loss 2895.2789\n",
      "Iteration 2130 : Loss 2895.2585\n",
      "Iteration 2140 : Loss 2895.2380\n",
      "Iteration 2150 : Loss 2895.2176\n",
      "Iteration 2160 : Loss 2895.1973\n",
      "Iteration 2170 : Loss 2895.1769\n",
      "Iteration 2180 : Loss 2895.1566\n",
      "Iteration 2190 : Loss 2895.1363\n",
      "Iteration 2200 : Loss 2895.1160\n",
      "Iteration 2210 : Loss 2895.0958\n",
      "Iteration 2220 : Loss 2895.0755\n",
      "Iteration 2230 : Loss 2895.0553\n",
      "Iteration 2240 : Loss 2895.0352\n",
      "Iteration 2250 : Loss 2895.0150\n",
      "Iteration 2260 : Loss 2894.9949\n",
      "Iteration 2270 : Loss 2894.9747\n",
      "Iteration 2280 : Loss 2894.9546\n",
      "Iteration 2290 : Loss 2894.9346\n",
      "Iteration 2300 : Loss 2894.9145\n",
      "Iteration 2310 : Loss 2894.8945\n",
      "Iteration 2320 : Loss 2894.8745\n",
      "Iteration 2330 : Loss 2894.8545\n",
      "Iteration 2340 : Loss 2894.8345\n",
      "Iteration 2350 : Loss 2894.8145\n",
      "Iteration 2360 : Loss 2894.7946\n",
      "Iteration 2370 : Loss 2894.7747\n",
      "Iteration 2380 : Loss 2894.7548\n",
      "Iteration 2390 : Loss 2894.7349\n",
      "Iteration 2400 : Loss 2894.7150\n",
      "Iteration 2410 : Loss 2894.6952\n",
      "Iteration 2420 : Loss 2894.6754\n",
      "Iteration 2430 : Loss 2894.6555\n",
      "Iteration 2440 : Loss 2894.6358\n",
      "Iteration 2450 : Loss 2894.6160\n",
      "Iteration 2460 : Loss 2894.5962\n",
      "Iteration 2470 : Loss 2894.5765\n",
      "Iteration 2480 : Loss 2894.5568\n",
      "Iteration 2490 : Loss 2894.5371\n",
      "Iteration 2500 : Loss 2894.5174\n",
      "Iteration 2510 : Loss 2894.4977\n",
      "Iteration 2520 : Loss 2894.4781\n",
      "Iteration 2530 : Loss 2894.4584\n",
      "Iteration 2540 : Loss 2894.4388\n",
      "Iteration 2550 : Loss 2894.4192\n",
      "Iteration 2560 : Loss 2894.3996\n",
      "Iteration 2570 : Loss 2894.3801\n",
      "Iteration 2580 : Loss 2894.3605\n",
      "Iteration 2590 : Loss 2894.3410\n",
      "Iteration 2600 : Loss 2894.3215\n",
      "Iteration 2610 : Loss 2894.3020\n",
      "Iteration 2620 : Loss 2894.2825\n",
      "Iteration 2630 : Loss 2894.2630\n",
      "Iteration 2640 : Loss 2894.2436\n",
      "Iteration 2650 : Loss 2894.2241\n",
      "Iteration 2660 : Loss 2894.2047\n",
      "Iteration 2670 : Loss 2894.1853\n",
      "Iteration 2680 : Loss 2894.1659\n",
      "Iteration 2690 : Loss 2894.1466\n",
      "Iteration 2700 : Loss 2894.1272\n",
      "Iteration 2710 : Loss 2894.1079\n",
      "Iteration 2720 : Loss 2894.0885\n",
      "Iteration 2730 : Loss 2894.0692\n",
      "Iteration 2740 : Loss 2894.0499\n",
      "Iteration 2750 : Loss 2894.0307\n",
      "Iteration 2760 : Loss 2894.0114\n",
      "Iteration 2770 : Loss 2893.9922\n",
      "Iteration 2780 : Loss 2893.9729\n",
      "Iteration 2790 : Loss 2893.9537\n",
      "Iteration 2800 : Loss 2893.9345\n",
      "Iteration 2810 : Loss 2893.9153\n",
      "Iteration 2820 : Loss 2893.8962\n",
      "Iteration 2830 : Loss 2893.8770\n",
      "Iteration 2840 : Loss 2893.8579\n",
      "Iteration 2850 : Loss 2893.8387\n",
      "Iteration 2860 : Loss 2893.8196\n",
      "Iteration 2870 : Loss 2893.8005\n",
      "Iteration 2880 : Loss 2893.7815\n",
      "Iteration 2890 : Loss 2893.7624\n",
      "Iteration 2900 : Loss 2893.7433\n",
      "Iteration 2910 : Loss 2893.7243\n",
      "Iteration 2920 : Loss 2893.7053\n",
      "Iteration 2930 : Loss 2893.6863\n",
      "Iteration 2940 : Loss 2893.6673\n",
      "Iteration 2950 : Loss 2893.6483\n",
      "Iteration 2960 : Loss 2893.6294\n",
      "Iteration 2970 : Loss 2893.6104\n",
      "Iteration 2980 : Loss 2893.5915\n",
      "Iteration 2990 : Loss 2893.5726\n",
      "Iteration 3000 : Loss 2893.5537\n",
      "Iteration 3010 : Loss 2893.5348\n",
      "Iteration 3020 : Loss 2893.5159\n",
      "Iteration 3030 : Loss 2893.4971\n",
      "Iteration 3040 : Loss 2893.4782\n",
      "Iteration 3050 : Loss 2893.4594\n",
      "Iteration 3060 : Loss 2893.4406\n",
      "Iteration 3070 : Loss 2893.4218\n",
      "Iteration 3080 : Loss 2893.4030\n",
      "Iteration 3090 : Loss 2893.3842\n",
      "Iteration 3100 : Loss 2893.3655\n",
      "Iteration 3110 : Loss 2893.3468\n",
      "Iteration 3120 : Loss 2893.3280\n",
      "Iteration 3130 : Loss 2893.3093\n",
      "Iteration 3140 : Loss 2893.2906\n",
      "Iteration 3150 : Loss 2893.2719\n",
      "Iteration 3160 : Loss 2893.2533\n",
      "Iteration 3170 : Loss 2893.2346\n",
      "Iteration 3180 : Loss 2893.2160\n",
      "Iteration 3190 : Loss 2893.1974\n",
      "Iteration 3200 : Loss 2893.1788\n",
      "Iteration 3210 : Loss 2893.1602\n",
      "Iteration 3220 : Loss 2893.1416\n",
      "Iteration 3230 : Loss 2893.1230\n",
      "Iteration 3240 : Loss 2893.1045\n",
      "Iteration 3250 : Loss 2893.0859\n",
      "Iteration 3260 : Loss 2893.0674\n",
      "Iteration 3270 : Loss 2893.0489\n",
      "Iteration 3280 : Loss 2893.0304\n",
      "Iteration 3290 : Loss 2893.0119\n",
      "Iteration 3300 : Loss 2892.9935\n",
      "Iteration 3310 : Loss 2892.9750\n",
      "Iteration 3320 : Loss 2892.9566\n",
      "Iteration 3330 : Loss 2892.9382\n",
      "Iteration 3340 : Loss 2892.9198\n",
      "Iteration 3350 : Loss 2892.9014\n",
      "Iteration 3360 : Loss 2892.8830\n",
      "Iteration 3370 : Loss 2892.8646\n",
      "Iteration 3380 : Loss 2892.8463\n",
      "Iteration 3390 : Loss 2892.8279\n",
      "Iteration 3400 : Loss 2892.8096\n",
      "Iteration 3410 : Loss 2892.7913\n",
      "Iteration 3420 : Loss 2892.7730\n",
      "Iteration 3430 : Loss 2892.7547\n",
      "Iteration 3440 : Loss 2892.7365\n",
      "Iteration 3450 : Loss 2892.7182\n",
      "Iteration 3460 : Loss 2892.7000\n",
      "Iteration 3470 : Loss 2892.6817\n",
      "Iteration 3480 : Loss 2892.6635\n",
      "Iteration 3490 : Loss 2892.6453\n",
      "Iteration 3500 : Loss 2892.6272\n",
      "Iteration 3510 : Loss 2892.6090\n",
      "Iteration 3520 : Loss 2892.5908\n",
      "Iteration 3530 : Loss 2892.5727\n",
      "Iteration 3540 : Loss 2892.5546\n",
      "Iteration 3550 : Loss 2892.5365\n",
      "Iteration 3560 : Loss 2892.5184\n",
      "Iteration 3570 : Loss 2892.5003\n",
      "Iteration 3580 : Loss 2892.4822\n",
      "Iteration 3590 : Loss 2892.4641\n",
      "Iteration 3600 : Loss 2892.4461\n",
      "Iteration 3610 : Loss 2892.4281\n",
      "Iteration 3620 : Loss 2892.4101\n",
      "Iteration 3630 : Loss 2892.3921\n",
      "Iteration 3640 : Loss 2892.3741\n",
      "Iteration 3650 : Loss 2892.3561\n",
      "Iteration 3660 : Loss 2892.3381\n",
      "Iteration 3670 : Loss 2892.3202\n",
      "Iteration 3680 : Loss 2892.3023\n",
      "Iteration 3690 : Loss 2892.2843\n",
      "Iteration 3700 : Loss 2892.2664\n",
      "Iteration 3710 : Loss 2892.2485\n",
      "Iteration 3720 : Loss 2892.2307\n",
      "Iteration 3730 : Loss 2892.2128\n",
      "Iteration 3740 : Loss 2892.1949\n",
      "Iteration 3750 : Loss 2892.1771\n",
      "Iteration 3760 : Loss 2892.1593\n",
      "Iteration 3770 : Loss 2892.1415\n",
      "Iteration 3780 : Loss 2892.1237\n",
      "Iteration 3790 : Loss 2892.1059\n",
      "Iteration 3800 : Loss 2892.0881\n",
      "Iteration 3810 : Loss 2892.0704\n",
      "Iteration 3820 : Loss 2892.0526\n",
      "Iteration 3830 : Loss 2892.0349\n",
      "Iteration 3840 : Loss 2892.0172\n",
      "Iteration 3850 : Loss 2891.9995\n",
      "Iteration 3860 : Loss 2891.9818\n",
      "Iteration 3870 : Loss 2891.9641\n",
      "Iteration 3880 : Loss 2891.9465\n",
      "Iteration 3890 : Loss 2891.9288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3900 : Loss 2891.9112\n",
      "Iteration 3910 : Loss 2891.8936\n",
      "Iteration 3920 : Loss 2891.8760\n",
      "Iteration 3930 : Loss 2891.8584\n",
      "Iteration 3940 : Loss 2891.8408\n",
      "Iteration 3950 : Loss 2891.8232\n",
      "Iteration 3960 : Loss 2891.8057\n",
      "Iteration 3970 : Loss 2891.7881\n",
      "Iteration 3980 : Loss 2891.7706\n",
      "Iteration 3990 : Loss 2891.7531\n",
      "Iteration 4000 : Loss 2891.7356\n",
      "Iteration 4010 : Loss 2891.7181\n",
      "Iteration 4020 : Loss 2891.7006\n",
      "Iteration 4030 : Loss 2891.6832\n",
      "Iteration 4040 : Loss 2891.6657\n",
      "Iteration 4050 : Loss 2891.6483\n",
      "Iteration 4060 : Loss 2891.6309\n",
      "Iteration 4070 : Loss 2891.6135\n",
      "Iteration 4080 : Loss 2891.5961\n",
      "Iteration 4090 : Loss 2891.5787\n",
      "Iteration 4100 : Loss 2891.5613\n",
      "Iteration 4110 : Loss 2891.5440\n",
      "Iteration 4120 : Loss 2891.5266\n",
      "Iteration 4130 : Loss 2891.5093\n",
      "Iteration 4140 : Loss 2891.4920\n",
      "Iteration 4150 : Loss 2891.4747\n",
      "Iteration 4160 : Loss 2891.4574\n",
      "Iteration 4170 : Loss 2891.4401\n",
      "Iteration 4180 : Loss 2891.4229\n",
      "Iteration 4190 : Loss 2891.4056\n",
      "Iteration 4200 : Loss 2891.3884\n",
      "Iteration 4210 : Loss 2891.3712\n",
      "Iteration 4220 : Loss 2891.3540\n",
      "Iteration 4230 : Loss 2891.3368\n",
      "Iteration 4240 : Loss 2891.3196\n",
      "Iteration 4250 : Loss 2891.3024\n",
      "Iteration 4260 : Loss 2891.2853\n",
      "Iteration 4270 : Loss 2891.2681\n",
      "Iteration 4280 : Loss 2891.2510\n",
      "Iteration 4290 : Loss 2891.2339\n",
      "Iteration 4300 : Loss 2891.2168\n",
      "Iteration 4310 : Loss 2891.1997\n",
      "Iteration 4320 : Loss 2891.1826\n",
      "Iteration 4330 : Loss 2891.1655\n",
      "Iteration 4340 : Loss 2891.1485\n",
      "Iteration 4350 : Loss 2891.1315\n",
      "Iteration 4360 : Loss 2891.1144\n",
      "Iteration 4370 : Loss 2891.0974\n",
      "Iteration 4380 : Loss 2891.0804\n",
      "Iteration 4390 : Loss 2891.0634\n",
      "Iteration 4400 : Loss 2891.0465\n",
      "Iteration 4410 : Loss 2891.0295\n",
      "Iteration 4420 : Loss 2891.0126\n",
      "Iteration 4430 : Loss 2890.9956\n",
      "Iteration 4440 : Loss 2890.9787\n",
      "Iteration 4450 : Loss 2890.9618\n",
      "Iteration 4460 : Loss 2890.9449\n",
      "Iteration 4470 : Loss 2890.9280\n",
      "Iteration 4480 : Loss 2890.9111\n",
      "Iteration 4490 : Loss 2890.8943\n",
      "Iteration 4500 : Loss 2890.8774\n",
      "Iteration 4510 : Loss 2890.8606\n",
      "Iteration 4520 : Loss 2890.8438\n",
      "Iteration 4530 : Loss 2890.8270\n",
      "Iteration 4540 : Loss 2890.8102\n",
      "Iteration 4550 : Loss 2890.7934\n",
      "Iteration 4560 : Loss 2890.7766\n",
      "Iteration 4570 : Loss 2890.7599\n",
      "Iteration 4580 : Loss 2890.7431\n",
      "Iteration 4590 : Loss 2890.7264\n",
      "Iteration 4600 : Loss 2890.7097\n",
      "Iteration 4610 : Loss 2890.6930\n",
      "Iteration 4620 : Loss 2890.6763\n",
      "Iteration 4630 : Loss 2890.6596\n",
      "Iteration 4640 : Loss 2890.6430\n",
      "Iteration 4650 : Loss 2890.6263\n",
      "Iteration 4660 : Loss 2890.6097\n",
      "Iteration 4670 : Loss 2890.5930\n",
      "Iteration 4680 : Loss 2890.5764\n",
      "Iteration 4690 : Loss 2890.5598\n",
      "Iteration 4700 : Loss 2890.5432\n",
      "Iteration 4710 : Loss 2890.5266\n",
      "Iteration 4720 : Loss 2890.5101\n",
      "Iteration 4730 : Loss 2890.4935\n",
      "Iteration 4740 : Loss 2890.4770\n",
      "Iteration 4750 : Loss 2890.4605\n",
      "Iteration 4760 : Loss 2890.4439\n",
      "Iteration 4770 : Loss 2890.4274\n",
      "Iteration 4780 : Loss 2890.4110\n",
      "Iteration 4790 : Loss 2890.3945\n",
      "Iteration 4800 : Loss 2890.3780\n",
      "Iteration 4810 : Loss 2890.3616\n",
      "Iteration 4820 : Loss 2890.3451\n",
      "Iteration 4830 : Loss 2890.3287\n",
      "Iteration 4840 : Loss 2890.3123\n",
      "Iteration 4850 : Loss 2890.2959\n",
      "Iteration 4860 : Loss 2890.2795\n",
      "Iteration 4870 : Loss 2890.2631\n",
      "Iteration 4880 : Loss 2890.2467\n",
      "Iteration 4890 : Loss 2890.2304\n",
      "Iteration 4900 : Loss 2890.2141\n",
      "Iteration 4910 : Loss 2890.1977\n",
      "Iteration 4920 : Loss 2890.1814\n",
      "Iteration 4930 : Loss 2890.1651\n",
      "Iteration 4940 : Loss 2890.1488\n",
      "Iteration 4950 : Loss 2890.1325\n",
      "Iteration 4960 : Loss 2890.1163\n",
      "Iteration 4970 : Loss 2890.1000\n",
      "Iteration 4980 : Loss 2890.0838\n",
      "Iteration 4990 : Loss 2890.0676\n",
      "Iteration 5000 : Loss 2890.0513\n",
      "Iteration 5010 : Loss 2890.0351\n",
      "Iteration 5020 : Loss 2890.0189\n",
      "Iteration 5030 : Loss 2890.0028\n",
      "Iteration 5040 : Loss 2889.9866\n",
      "Iteration 5050 : Loss 2889.9704\n",
      "Iteration 5060 : Loss 2889.9543\n",
      "Iteration 5070 : Loss 2889.9382\n",
      "Iteration 5080 : Loss 2889.9221\n",
      "Iteration 5090 : Loss 2889.9059\n",
      "Iteration 5100 : Loss 2889.8899\n",
      "Iteration 5110 : Loss 2889.8738\n",
      "Iteration 5120 : Loss 2889.8577\n",
      "Iteration 5130 : Loss 2889.8416\n",
      "Iteration 5140 : Loss 2889.8256\n",
      "Iteration 5150 : Loss 2889.8096\n",
      "Iteration 5160 : Loss 2889.7935\n",
      "Iteration 5170 : Loss 2889.7775\n",
      "Iteration 5180 : Loss 2889.7615\n",
      "Iteration 5190 : Loss 2889.7456\n",
      "Iteration 5200 : Loss 2889.7296\n",
      "Iteration 5210 : Loss 2889.7136\n",
      "Iteration 5220 : Loss 2889.6977\n",
      "Iteration 5230 : Loss 2889.6817\n",
      "Iteration 5240 : Loss 2889.6658\n",
      "Iteration 5250 : Loss 2889.6499\n",
      "Iteration 5260 : Loss 2889.6340\n",
      "Iteration 5270 : Loss 2889.6181\n",
      "Iteration 5280 : Loss 2889.6022\n",
      "Iteration 5290 : Loss 2889.5864\n",
      "Iteration 5300 : Loss 2889.5705\n",
      "Iteration 5310 : Loss 2889.5547\n",
      "Iteration 5320 : Loss 2889.5389\n",
      "Iteration 5330 : Loss 2889.5230\n",
      "Iteration 5340 : Loss 2889.5072\n",
      "Iteration 5350 : Loss 2889.4915\n",
      "Iteration 5360 : Loss 2889.4757\n",
      "Iteration 5370 : Loss 2889.4599\n",
      "Iteration 5380 : Loss 2889.4442\n",
      "Iteration 5390 : Loss 2889.4284\n",
      "Iteration 5400 : Loss 2889.4127\n",
      "Iteration 5410 : Loss 2889.3970\n",
      "Iteration 5420 : Loss 2889.3813\n",
      "Iteration 5430 : Loss 2889.3656\n",
      "Iteration 5440 : Loss 2889.3499\n",
      "Iteration 5450 : Loss 2889.3342\n",
      "Iteration 5460 : Loss 2889.3185\n",
      "Iteration 5470 : Loss 2889.3029\n",
      "Iteration 5480 : Loss 2889.2873\n",
      "Iteration 5490 : Loss 2889.2716\n",
      "Iteration 5500 : Loss 2889.2560\n",
      "Iteration 5510 : Loss 2889.2404\n",
      "Iteration 5520 : Loss 2889.2248\n",
      "Iteration 5530 : Loss 2889.2092\n",
      "Iteration 5540 : Loss 2889.1937\n",
      "Iteration 5550 : Loss 2889.1781\n",
      "Iteration 5560 : Loss 2889.1626\n",
      "Iteration 5570 : Loss 2889.1471\n",
      "Iteration 5580 : Loss 2889.1315\n",
      "Iteration 5590 : Loss 2889.1160\n",
      "Iteration 5600 : Loss 2889.1005\n",
      "Iteration 5610 : Loss 2889.0851\n",
      "Iteration 5620 : Loss 2889.0696\n",
      "Iteration 5630 : Loss 2889.0541\n",
      "Iteration 5640 : Loss 2889.0387\n",
      "Iteration 5650 : Loss 2889.0232\n",
      "Iteration 5660 : Loss 2889.0078\n",
      "Iteration 5670 : Loss 2888.9924\n",
      "Iteration 5680 : Loss 2888.9770\n",
      "Iteration 5690 : Loss 2888.9616\n",
      "Iteration 5700 : Loss 2888.9462\n",
      "Iteration 5710 : Loss 2888.9309\n",
      "Iteration 5720 : Loss 2888.9155\n",
      "Iteration 5730 : Loss 2888.9002\n",
      "Iteration 5740 : Loss 2888.8848\n",
      "Iteration 5750 : Loss 2888.8695\n",
      "Iteration 5760 : Loss 2888.8542\n",
      "Iteration 5770 : Loss 2888.8389\n",
      "Iteration 5780 : Loss 2888.8236\n",
      "Iteration 5790 : Loss 2888.8084\n",
      "Iteration 5800 : Loss 2888.7931\n",
      "Iteration 5810 : Loss 2888.7778\n",
      "Iteration 5820 : Loss 2888.7626\n",
      "Iteration 5830 : Loss 2888.7474\n",
      "Iteration 5840 : Loss 2888.7322\n",
      "Iteration 5850 : Loss 2888.7170\n",
      "Iteration 5860 : Loss 2888.7018\n",
      "Iteration 5870 : Loss 2888.6866\n",
      "Iteration 5880 : Loss 2888.6714\n",
      "Iteration 5890 : Loss 2888.6563\n",
      "Iteration 5900 : Loss 2888.6411\n",
      "Iteration 5910 : Loss 2888.6260\n",
      "Iteration 5920 : Loss 2888.6108\n",
      "Iteration 5930 : Loss 2888.5957\n",
      "Iteration 5940 : Loss 2888.5806\n",
      "Iteration 5950 : Loss 2888.5655\n",
      "Iteration 5960 : Loss 2888.5505\n",
      "Iteration 5970 : Loss 2888.5354\n",
      "Iteration 5980 : Loss 2888.5203\n",
      "Iteration 5990 : Loss 2888.5053\n",
      "Iteration 6000 : Loss 2888.4903\n",
      "Iteration 6010 : Loss 2888.4752\n",
      "Iteration 6020 : Loss 2888.4602\n",
      "Iteration 6030 : Loss 2888.4452\n",
      "Iteration 6040 : Loss 2888.4303\n",
      "Iteration 6050 : Loss 2888.4153\n",
      "Iteration 6060 : Loss 2888.4003\n",
      "Iteration 6070 : Loss 2888.3854\n",
      "Iteration 6080 : Loss 2888.3704\n",
      "Iteration 6090 : Loss 2888.3555\n",
      "Iteration 6100 : Loss 2888.3406\n",
      "Iteration 6110 : Loss 2888.3257\n",
      "Iteration 6120 : Loss 2888.3108\n",
      "Iteration 6130 : Loss 2888.2959\n",
      "Iteration 6140 : Loss 2888.2810\n",
      "Iteration 6150 : Loss 2888.2661\n",
      "Iteration 6160 : Loss 2888.2513\n",
      "Iteration 6170 : Loss 2888.2364\n",
      "Iteration 6180 : Loss 2888.2216\n",
      "Iteration 6190 : Loss 2888.2068\n",
      "Iteration 6200 : Loss 2888.1920\n",
      "Iteration 6210 : Loss 2888.1772\n",
      "Iteration 6220 : Loss 2888.1624\n",
      "Iteration 6230 : Loss 2888.1476\n",
      "Iteration 6240 : Loss 2888.1329\n",
      "Iteration 6250 : Loss 2888.1181\n",
      "Iteration 6260 : Loss 2888.1034\n",
      "Iteration 6270 : Loss 2888.0887\n",
      "Iteration 6280 : Loss 2888.0739\n",
      "Iteration 6290 : Loss 2888.0592\n",
      "Iteration 6300 : Loss 2888.0445\n",
      "Iteration 6310 : Loss 2888.0299\n",
      "Iteration 6320 : Loss 2888.0152\n",
      "Iteration 6330 : Loss 2888.0005\n",
      "Iteration 6340 : Loss 2887.9859\n",
      "Iteration 6350 : Loss 2887.9712\n",
      "Iteration 6360 : Loss 2887.9566\n",
      "Iteration 6370 : Loss 2887.9420\n",
      "Iteration 6380 : Loss 2887.9274\n",
      "Iteration 6390 : Loss 2887.9128\n",
      "Iteration 6400 : Loss 2887.8982\n",
      "Iteration 6410 : Loss 2887.8836\n",
      "Iteration 6420 : Loss 2887.8691\n",
      "Iteration 6430 : Loss 2887.8545\n",
      "Iteration 6440 : Loss 2887.8400\n",
      "Iteration 6450 : Loss 2887.8254\n",
      "Iteration 6460 : Loss 2887.8109\n",
      "Iteration 6470 : Loss 2887.7964\n",
      "Iteration 6480 : Loss 2887.7819\n",
      "Iteration 6490 : Loss 2887.7674\n",
      "Iteration 6500 : Loss 2887.7529\n",
      "Iteration 6510 : Loss 2887.7385\n",
      "Iteration 6520 : Loss 2887.7240\n",
      "Iteration 6530 : Loss 2887.7096\n",
      "Iteration 6540 : Loss 2887.6952\n",
      "Iteration 6550 : Loss 2887.6807\n",
      "Iteration 6560 : Loss 2887.6663\n",
      "Iteration 6570 : Loss 2887.6519\n",
      "Iteration 6580 : Loss 2887.6375\n",
      "Iteration 6590 : Loss 2887.6232\n",
      "Iteration 6600 : Loss 2887.6088\n",
      "Iteration 6610 : Loss 2887.5944\n",
      "Iteration 6620 : Loss 2887.5801\n",
      "Iteration 6630 : Loss 2887.5658\n",
      "Iteration 6640 : Loss 2887.5514\n",
      "Iteration 6650 : Loss 2887.5371\n",
      "Iteration 6660 : Loss 2887.5228\n",
      "Iteration 6670 : Loss 2887.5085\n",
      "Iteration 6680 : Loss 2887.4943\n",
      "Iteration 6690 : Loss 2887.4800\n",
      "Iteration 6700 : Loss 2887.4657\n",
      "Iteration 6710 : Loss 2887.4515\n",
      "Iteration 6720 : Loss 2887.4373\n",
      "Iteration 6730 : Loss 2887.4230\n",
      "Iteration 6740 : Loss 2887.4088\n",
      "Iteration 6750 : Loss 2887.3946\n",
      "Iteration 6760 : Loss 2887.3804\n",
      "Iteration 6770 : Loss 2887.3662\n",
      "Iteration 6780 : Loss 2887.3521\n",
      "Iteration 6790 : Loss 2887.3379\n",
      "Iteration 6800 : Loss 2887.3238\n",
      "Iteration 6810 : Loss 2887.3096\n",
      "Iteration 6820 : Loss 2887.2955\n",
      "Iteration 6830 : Loss 2887.2814\n",
      "Iteration 6840 : Loss 2887.2673\n",
      "Iteration 6850 : Loss 2887.2532\n",
      "Iteration 6860 : Loss 2887.2391\n",
      "Iteration 6870 : Loss 2887.2250\n",
      "Iteration 6880 : Loss 2887.2109\n",
      "Iteration 6890 : Loss 2887.1969\n",
      "Iteration 6900 : Loss 2887.1828\n",
      "Iteration 6910 : Loss 2887.1688\n",
      "Iteration 6920 : Loss 2887.1548\n",
      "Iteration 6930 : Loss 2887.1408\n",
      "Iteration 6940 : Loss 2887.1268\n",
      "Iteration 6950 : Loss 2887.1128\n",
      "Iteration 6960 : Loss 2887.0988\n",
      "Iteration 6970 : Loss 2887.0848\n",
      "Iteration 6980 : Loss 2887.0709\n",
      "Iteration 6990 : Loss 2887.0569\n",
      "Iteration 7000 : Loss 2887.0430\n",
      "Iteration 7010 : Loss 2887.0290\n",
      "Iteration 7020 : Loss 2887.0151\n",
      "Iteration 7030 : Loss 2887.0012\n",
      "Iteration 7040 : Loss 2886.9873\n",
      "Iteration 7050 : Loss 2886.9734\n",
      "Iteration 7060 : Loss 2886.9596\n",
      "Iteration 7070 : Loss 2886.9457\n",
      "Iteration 7080 : Loss 2886.9318\n",
      "Iteration 7090 : Loss 2886.9180\n",
      "Iteration 7100 : Loss 2886.9042\n",
      "Iteration 7110 : Loss 2886.8903\n",
      "Iteration 7120 : Loss 2886.8765\n",
      "Iteration 7130 : Loss 2886.8627\n",
      "Iteration 7140 : Loss 2886.8489\n",
      "Iteration 7150 : Loss 2886.8352\n",
      "Iteration 7160 : Loss 2886.8214\n",
      "Iteration 7170 : Loss 2886.8076\n",
      "Iteration 7180 : Loss 2886.7939\n",
      "Iteration 7190 : Loss 2886.7801\n",
      "Iteration 7200 : Loss 2886.7664\n",
      "Iteration 7210 : Loss 2886.7527\n",
      "Iteration 7220 : Loss 2886.7390\n",
      "Iteration 7230 : Loss 2886.7253\n",
      "Iteration 7240 : Loss 2886.7116\n",
      "Iteration 7250 : Loss 2886.6979\n",
      "Iteration 7260 : Loss 2886.6843\n",
      "Iteration 7270 : Loss 2886.6706\n",
      "Iteration 7280 : Loss 2886.6569\n",
      "Iteration 7290 : Loss 2886.6433\n",
      "Iteration 7300 : Loss 2886.6297\n",
      "Iteration 7310 : Loss 2886.6161\n",
      "Iteration 7320 : Loss 2886.6025\n",
      "Iteration 7330 : Loss 2886.5889\n",
      "Iteration 7340 : Loss 2886.5753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7350 : Loss 2886.5617\n",
      "Iteration 7360 : Loss 2886.5481\n",
      "Iteration 7370 : Loss 2886.5346\n",
      "Iteration 7380 : Loss 2886.5210\n",
      "Iteration 7390 : Loss 2886.5075\n",
      "Iteration 7400 : Loss 2886.4940\n",
      "Iteration 7410 : Loss 2886.4805\n",
      "Iteration 7420 : Loss 2886.4670\n",
      "Iteration 7430 : Loss 2886.4535\n",
      "Iteration 7440 : Loss 2886.4400\n",
      "Iteration 7450 : Loss 2886.4265\n",
      "Iteration 7460 : Loss 2886.4131\n",
      "Iteration 7470 : Loss 2886.3996\n",
      "Iteration 7480 : Loss 2886.3862\n",
      "Iteration 7490 : Loss 2886.3727\n",
      "Iteration 7500 : Loss 2886.3593\n",
      "Iteration 7510 : Loss 2886.3459\n",
      "Iteration 7520 : Loss 2886.3325\n",
      "Iteration 7530 : Loss 2886.3191\n",
      "Iteration 7540 : Loss 2886.3057\n",
      "Iteration 7550 : Loss 2886.2924\n",
      "Iteration 7560 : Loss 2886.2790\n",
      "Iteration 7570 : Loss 2886.2656\n",
      "Iteration 7580 : Loss 2886.2523\n",
      "Iteration 7590 : Loss 2886.2390\n",
      "Iteration 7600 : Loss 2886.2256\n",
      "Iteration 7610 : Loss 2886.2123\n",
      "Iteration 7620 : Loss 2886.1990\n",
      "Iteration 7630 : Loss 2886.1857\n",
      "Iteration 7640 : Loss 2886.1725\n",
      "Iteration 7650 : Loss 2886.1592\n",
      "Iteration 7660 : Loss 2886.1459\n",
      "Iteration 7670 : Loss 2886.1327\n",
      "Iteration 7680 : Loss 2886.1194\n",
      "Iteration 7690 : Loss 2886.1062\n",
      "Iteration 7700 : Loss 2886.0930\n",
      "Iteration 7710 : Loss 2886.0798\n",
      "Iteration 7720 : Loss 2886.0666\n",
      "Iteration 7730 : Loss 2886.0534\n",
      "Iteration 7740 : Loss 2886.0402\n",
      "Iteration 7750 : Loss 2886.0270\n",
      "Iteration 7760 : Loss 2886.0139\n",
      "Iteration 7770 : Loss 2886.0007\n",
      "Iteration 7780 : Loss 2885.9876\n",
      "Iteration 7790 : Loss 2885.9744\n",
      "Iteration 7800 : Loss 2885.9613\n",
      "Iteration 7810 : Loss 2885.9482\n",
      "Iteration 7820 : Loss 2885.9351\n",
      "Iteration 7830 : Loss 2885.9220\n",
      "Iteration 7840 : Loss 2885.9089\n",
      "Iteration 7850 : Loss 2885.8959\n",
      "Iteration 7860 : Loss 2885.8828\n",
      "Iteration 7870 : Loss 2885.8698\n",
      "Iteration 7880 : Loss 2885.8567\n",
      "Iteration 7890 : Loss 2885.8437\n",
      "Iteration 7900 : Loss 2885.8307\n",
      "Iteration 7910 : Loss 2885.8176\n",
      "Iteration 7920 : Loss 2885.8046\n",
      "Iteration 7930 : Loss 2885.7916\n",
      "Iteration 7940 : Loss 2885.7787\n",
      "Iteration 7950 : Loss 2885.7657\n",
      "Iteration 7960 : Loss 2885.7527\n",
      "Iteration 7970 : Loss 2885.7398\n",
      "Iteration 7980 : Loss 2885.7268\n",
      "Iteration 7990 : Loss 2885.7139\n",
      "Iteration 8000 : Loss 2885.7010\n",
      "Iteration 8010 : Loss 2885.6880\n",
      "Iteration 8020 : Loss 2885.6751\n",
      "Iteration 8030 : Loss 2885.6622\n",
      "Iteration 8040 : Loss 2885.6494\n",
      "Iteration 8050 : Loss 2885.6365\n",
      "Iteration 8060 : Loss 2885.6236\n",
      "Iteration 8070 : Loss 2885.6108\n",
      "Iteration 8080 : Loss 2885.5979\n",
      "Iteration 8090 : Loss 2885.5851\n",
      "Iteration 8100 : Loss 2885.5722\n",
      "Iteration 8110 : Loss 2885.5594\n",
      "Iteration 8120 : Loss 2885.5466\n",
      "Iteration 8130 : Loss 2885.5338\n",
      "Iteration 8140 : Loss 2885.5210\n",
      "Iteration 8150 : Loss 2885.5082\n",
      "Iteration 8160 : Loss 2885.4955\n",
      "Iteration 8170 : Loss 2885.4827\n",
      "Iteration 8180 : Loss 2885.4700\n",
      "Iteration 8190 : Loss 2885.4572\n",
      "Iteration 8200 : Loss 2885.4445\n",
      "Iteration 8210 : Loss 2885.4318\n",
      "Iteration 8220 : Loss 2885.4191\n",
      "Iteration 8230 : Loss 2885.4064\n",
      "Iteration 8240 : Loss 2885.3937\n",
      "Iteration 8250 : Loss 2885.3810\n",
      "Iteration 8260 : Loss 2885.3683\n",
      "Iteration 8270 : Loss 2885.3556\n",
      "Iteration 8280 : Loss 2885.3430\n",
      "Iteration 8290 : Loss 2885.3303\n",
      "Iteration 8300 : Loss 2885.3177\n",
      "Iteration 8310 : Loss 2885.3051\n",
      "Iteration 8320 : Loss 2885.2925\n",
      "Iteration 8330 : Loss 2885.2799\n",
      "Iteration 8340 : Loss 2885.2673\n",
      "Iteration 8350 : Loss 2885.2547\n",
      "Iteration 8360 : Loss 2885.2421\n",
      "Iteration 8370 : Loss 2885.2295\n",
      "Iteration 8380 : Loss 2885.2170\n",
      "Iteration 8390 : Loss 2885.2044\n",
      "Iteration 8400 : Loss 2885.1919\n",
      "Iteration 8410 : Loss 2885.1793\n",
      "Iteration 8420 : Loss 2885.1668\n",
      "Iteration 8430 : Loss 2885.1543\n",
      "Iteration 8440 : Loss 2885.1418\n",
      "Iteration 8450 : Loss 2885.1293\n",
      "Iteration 8460 : Loss 2885.1168\n",
      "Iteration 8470 : Loss 2885.1043\n",
      "Iteration 8480 : Loss 2885.0919\n",
      "Iteration 8490 : Loss 2885.0794\n",
      "Iteration 8500 : Loss 2885.0670\n",
      "Iteration 8510 : Loss 2885.0545\n",
      "Iteration 8520 : Loss 2885.0421\n",
      "Iteration 8530 : Loss 2885.0297\n",
      "Iteration 8540 : Loss 2885.0173\n",
      "Iteration 8550 : Loss 2885.0049\n",
      "Iteration 8560 : Loss 2884.9925\n",
      "Iteration 8570 : Loss 2884.9801\n",
      "Iteration 8580 : Loss 2884.9677\n",
      "Iteration 8590 : Loss 2884.9554\n",
      "Iteration 8600 : Loss 2884.9430\n",
      "Iteration 8610 : Loss 2884.9307\n",
      "Iteration 8620 : Loss 2884.9183\n",
      "Iteration 8630 : Loss 2884.9060\n",
      "Iteration 8640 : Loss 2884.8937\n",
      "Iteration 8650 : Loss 2884.8814\n",
      "Iteration 8660 : Loss 2884.8691\n",
      "Iteration 8670 : Loss 2884.8568\n",
      "Iteration 8680 : Loss 2884.8445\n",
      "Iteration 8690 : Loss 2884.8323\n",
      "Iteration 8700 : Loss 2884.8200\n",
      "Iteration 8710 : Loss 2884.8078\n",
      "Iteration 8720 : Loss 2884.7955\n",
      "Iteration 8730 : Loss 2884.7833\n",
      "Iteration 8740 : Loss 2884.7711\n",
      "Iteration 8750 : Loss 2884.7588\n",
      "Iteration 8760 : Loss 2884.7466\n",
      "Iteration 8770 : Loss 2884.7344\n",
      "Iteration 8780 : Loss 2884.7222\n",
      "Iteration 8790 : Loss 2884.7101\n",
      "Iteration 8800 : Loss 2884.6979\n",
      "Iteration 8810 : Loss 2884.6857\n",
      "Iteration 8820 : Loss 2884.6736\n",
      "Iteration 8830 : Loss 2884.6615\n",
      "Iteration 8840 : Loss 2884.6493\n",
      "Iteration 8850 : Loss 2884.6372\n",
      "Iteration 8860 : Loss 2884.6251\n",
      "Iteration 8870 : Loss 2884.6130\n",
      "Iteration 8880 : Loss 2884.6009\n",
      "Iteration 8890 : Loss 2884.5888\n",
      "Iteration 8900 : Loss 2884.5767\n",
      "Iteration 8910 : Loss 2884.5647\n",
      "Iteration 8920 : Loss 2884.5526\n",
      "Iteration 8930 : Loss 2884.5405\n",
      "Iteration 8940 : Loss 2884.5285\n",
      "Iteration 8950 : Loss 2884.5165\n",
      "Iteration 8960 : Loss 2884.5045\n",
      "Iteration 8970 : Loss 2884.4924\n",
      "Iteration 8980 : Loss 2884.4804\n",
      "Iteration 8990 : Loss 2884.4684\n",
      "Iteration 9000 : Loss 2884.4565\n",
      "Iteration 9010 : Loss 2884.4445\n",
      "Iteration 9020 : Loss 2884.4325\n",
      "Iteration 9030 : Loss 2884.4205\n",
      "Iteration 9040 : Loss 2884.4086\n",
      "Iteration 9050 : Loss 2884.3967\n",
      "Iteration 9060 : Loss 2884.3847\n",
      "Iteration 9070 : Loss 2884.3728\n",
      "Iteration 9080 : Loss 2884.3609\n",
      "Iteration 9090 : Loss 2884.3490\n",
      "Iteration 9100 : Loss 2884.3371\n",
      "Iteration 9110 : Loss 2884.3252\n",
      "Iteration 9120 : Loss 2884.3133\n",
      "Iteration 9130 : Loss 2884.3014\n",
      "Iteration 9140 : Loss 2884.2896\n",
      "Iteration 9150 : Loss 2884.2777\n",
      "Iteration 9160 : Loss 2884.2659\n",
      "Iteration 9170 : Loss 2884.2541\n",
      "Iteration 9180 : Loss 2884.2422\n",
      "Iteration 9190 : Loss 2884.2304\n",
      "Iteration 9200 : Loss 2884.2186\n",
      "Iteration 9210 : Loss 2884.2068\n",
      "Iteration 9220 : Loss 2884.1950\n",
      "Iteration 9230 : Loss 2884.1832\n",
      "Iteration 9240 : Loss 2884.1715\n",
      "Iteration 9250 : Loss 2884.1597\n",
      "Iteration 9260 : Loss 2884.1480\n",
      "Iteration 9270 : Loss 2884.1362\n",
      "Iteration 9280 : Loss 2884.1245\n",
      "Iteration 9290 : Loss 2884.1127\n",
      "Iteration 9300 : Loss 2884.1010\n",
      "Iteration 9310 : Loss 2884.0893\n",
      "Iteration 9320 : Loss 2884.0776\n",
      "Iteration 9330 : Loss 2884.0659\n",
      "Iteration 9340 : Loss 2884.0542\n",
      "Iteration 9350 : Loss 2884.0426\n",
      "Iteration 9360 : Loss 2884.0309\n",
      "Iteration 9370 : Loss 2884.0192\n",
      "Iteration 9380 : Loss 2884.0076\n",
      "Iteration 9390 : Loss 2883.9960\n",
      "Iteration 9400 : Loss 2883.9843\n",
      "Iteration 9410 : Loss 2883.9727\n",
      "Iteration 9420 : Loss 2883.9611\n",
      "Iteration 9430 : Loss 2883.9495\n",
      "Iteration 9440 : Loss 2883.9379\n",
      "Iteration 9450 : Loss 2883.9263\n",
      "Iteration 9460 : Loss 2883.9147\n",
      "Iteration 9470 : Loss 2883.9032\n",
      "Iteration 9480 : Loss 2883.8916\n",
      "Iteration 9490 : Loss 2883.8800\n",
      "Iteration 9500 : Loss 2883.8685\n",
      "Iteration 9510 : Loss 2883.8570\n",
      "Iteration 9520 : Loss 2883.8454\n",
      "Iteration 9530 : Loss 2883.8339\n",
      "Iteration 9540 : Loss 2883.8224\n",
      "Iteration 9550 : Loss 2883.8109\n",
      "Iteration 9560 : Loss 2883.7994\n",
      "Iteration 9570 : Loss 2883.7879\n",
      "Iteration 9580 : Loss 2883.7765\n",
      "Iteration 9590 : Loss 2883.7650\n",
      "Iteration 9600 : Loss 2883.7535\n",
      "Iteration 9610 : Loss 2883.7421\n",
      "Iteration 9620 : Loss 2883.7307\n",
      "Iteration 9630 : Loss 2883.7192\n",
      "Iteration 9640 : Loss 2883.7078\n",
      "Iteration 9650 : Loss 2883.6964\n",
      "Iteration 9660 : Loss 2883.6850\n",
      "Iteration 9670 : Loss 2883.6736\n",
      "Iteration 9680 : Loss 2883.6622\n",
      "Iteration 9690 : Loss 2883.6508\n",
      "Iteration 9700 : Loss 2883.6395\n",
      "Iteration 9710 : Loss 2883.6281\n",
      "Iteration 9720 : Loss 2883.6167\n",
      "Iteration 9730 : Loss 2883.6054\n",
      "Iteration 9740 : Loss 2883.5941\n",
      "Iteration 9750 : Loss 2883.5827\n",
      "Iteration 9760 : Loss 2883.5714\n",
      "Iteration 9770 : Loss 2883.5601\n",
      "Iteration 9780 : Loss 2883.5488\n",
      "Iteration 9790 : Loss 2883.5375\n",
      "Iteration 9800 : Loss 2883.5262\n",
      "Iteration 9810 : Loss 2883.5149\n",
      "Iteration 9820 : Loss 2883.5037\n",
      "Iteration 9830 : Loss 2883.4924\n",
      "Iteration 9840 : Loss 2883.4812\n",
      "Iteration 9850 : Loss 2883.4699\n",
      "Iteration 9860 : Loss 2883.4587\n",
      "Iteration 9870 : Loss 2883.4475\n",
      "Iteration 9880 : Loss 2883.4362\n",
      "Iteration 9890 : Loss 2883.4250\n",
      "Iteration 9900 : Loss 2883.4138\n",
      "Iteration 9910 : Loss 2883.4026\n",
      "Iteration 9920 : Loss 2883.3915\n",
      "Iteration 9930 : Loss 2883.3803\n",
      "Iteration 9940 : Loss 2883.3691\n",
      "Iteration 9950 : Loss 2883.3580\n",
      "Iteration 9960 : Loss 2883.3468\n",
      "Iteration 9970 : Loss 2883.3357\n",
      "Iteration 9980 : Loss 2883.3245\n",
      "Iteration 9990 : Loss 2883.3134\n",
      "Iteration 10000 : Loss 2883.3023\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -=  LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6131d4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f0e82d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  41.23890916, -240.70280301,  553.982307  ,  344.03789852,\n",
       "        -334.9974511 ,   50.43646729, -104.54366614,  192.41336116,\n",
       "         507.78615579,   52.93104052]),\n",
       " 151.31522265203205)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "089e7771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2886.571763755155"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79528ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcfe99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
